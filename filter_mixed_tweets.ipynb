{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "# This regex implementation is backwards-compatible with the standard â€˜reâ€™ module, but offers additional functionality.\n",
    "import regex\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(data_file):\n",
    "    text_data = list()\n",
    "    labels = list()\n",
    "    infile = open(data_file, encoding='utf-8')\n",
    "    for line in infile:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        label, text = line.split('\\t')\n",
    "        text_data.append(text)\n",
    "        labels.append(label)\n",
    "    return text_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emo_from_text(text):\n",
    "    emoji_list = []\n",
    "    tokens = regex.findall(r'\\X', text)\n",
    "    for word in tokens:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(lex_file):\n",
    "    emoji_lex = open(lex_file, encoding='utf-8').read().split('\\n')\n",
    "    return emoji_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_emo_in_lex(emos, lex):\n",
    "    return any([emo in lex for emo in emos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mix(pos_data, neg_data):\n",
    "    new_pos = list()\n",
    "    new_neg = list()\n",
    "    mixed = list()\n",
    "    all_tweets = list()\n",
    "    for tweet in pos_data:\n",
    "        emojis = extract_emo_from_text(tweet)\n",
    "        if check_emo_in_lex(emojis, pos_lex) and check_emo_in_lex(emojis, neg_lex):\n",
    "            mixed.append(tweet)\n",
    "        else:\n",
    "            new_pos.append(tweet)\n",
    "    for tweet in neg_data:\n",
    "        emojis = extract_emo_from_text(tweet)\n",
    "        if check_emo_in_lex(emojis, pos_lex) and check_emo_in_lex(emojis, neg_lex):\n",
    "            mixed.append(tweet)\n",
    "        else:\n",
    "            new_neg.append(tweet)\n",
    "    print('after:', 'pos', len(new_pos), 'neg', len(new_neg), 'mix', len(mixed))\n",
    "    for tweet in new_pos:\n",
    "        all_tweets.append('pos\\t' + tweet)\n",
    "    for tweet in new_neg:\n",
    "        all_tweets.append('neg\\t' + tweet)\n",
    "    for tweet in mixed:\n",
    "        all_tweets.append('mix\\t' + tweet)\n",
    "    random.shuffle(all_tweets)\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tsv(outfile_name, tweets):\n",
    "    outfile = open(outfile_name, encoding='utf-8', mode='w')\n",
    "    for tweet in tweets:\n",
    "        outfile.write(tweet + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 351 neg: 124\n"
     ]
    }
   ],
   "source": [
    "pos_lex = load_lexicon('emoji_lex/positive_emoji_v3.txt')\n",
    "neg_lex = load_lexicon('emoji_lex/negative_emoji_v3.txt')\n",
    "print('pos:', len(pos_lex), 'neg:', len(neg_lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸŒŒ', 'â™›', 'â¥', 'ğŸŠ', '8)', 'ğŸ™‹', 'ğŸ‘£', 'ğŸ§¡', 'â™¨', ':â€™)', 'ğŸ“', '8-)', 'â„', 'ğŸ’', 'ğŸŠ', ':}', 'â˜€', 'ğŸ˜', 'ğŸ™Š', ';)', 'ğŸ†', 'ğŸŒ', 'ğŸ‘«', 'ğŸŒ', 'ğŸŒ½', ':>', 'ğŸ™‚', ':-3', 'ğŸ¥³', 'ğŸ“¹', 'â£', 'ğŸ˜', 'â™”', 'â…', 'ğŸ„', 'ğŸ‘', ':D', 'ğŸ’•', 'ğŸŒˆ', 'ğŸ‘œ', ';D', 'ğŸ‘¤', 'ğŸ¹', 'ğŸ’', '*)', 'ğŸ¾', 'ğŸš¼', 'xD', 'ğŸš„', 'â¤', 'ğŸ”', 'ğŸ˜®', 'ğŸ°', 'ğŸŒ›', 'ğŸ˜—', 'ğŸ˜Œ', 'ğŸ¤´', 'ğŸ˜¸', 'ğŸ™', 'ğŸŒ´', 'â¤ï¸', 'ğŸ¿', 'ğŸ‘›', 'âœ', 'ğŸ ', 'ğŸš¢', ':-,', 'ğŸŒ•', 'ğŸ”­', 'ğŸ‘‘', 'ğŸ‘¨', 'ğŸ’ƒ', 'ğŸµ', 'ğŸ’', 'ğŸŸ', '*-)', 'ğŸ’‹', 'ğŸ¡', 'ğŸ”', 'ğŸŒ·', 'ğŸ”†', 'ğŸˆ', 'ğŸ’Ÿ', ':-)', 'ğŸ˜‰', 'ğŸš¿', 'ğŸ°', ';]', 'ğŸ˜˜', '=3', 'ğŸ¹', 'âœ”', 'ğŸ‘', 'ğŸŒ ', 'ğŸ˜»', 'ğŸ†“', '=D', 'ğŸ¥', 'ğŸ', 'ğŸ‘Š', 'ğŸ¬', 'ğŸŒš', 'â™¡', 'ğŸ˜¬', 'ğŸ‰', 'ğŸ…', 'âœŒï¸', 'B-Ë†D', '<3', 'ğŸ¨', 'ğŸ˜', 'â˜ºï¸', 'ğŸ‰', 'â€', 'ğŸˆ', 'âœ“', 'ğŸ ', 'ğŸ¦', 'ğŸ’Œ', 'ğŸŒ±', 'ğŸª', 'ğŸ¤¡', ':Ë†)', 'ğŸ°', 'â˜º', 'ğŸ™ƒ', 'ğŸ’°', 'ğŸ“', ':-]', 'ğŸ’¼', 'ğŸ˜', 'ğŸ‘–', '=)', ':-}', 'ğŸ’›', 'ğŸŒ¾', 'ğŸ’', 'ğŸ¥', 'ğŸ’„', 'ğŸ¬', 'ğŸ', 'ğŸ‰', 'â›³', 'ğŸšµ', 'ğŸ©', 'ğŸ', 'ğŸ—»', 'ğŸŒ°', ':-*', 'ğŸ’³', 'ğŸ”…', 'â™ª', 'ğŸ™Œ', 'ğŸ£', 'ğŸ', 'ğŸ¼', 'ğŸŒ…', 'âœ”ï¸', 'ğŸ˜†', ':]', 'ğŸ€', 'ğŸ“º', ':-))', 'ğŸ˜š', 'ğŸ’š', 'ğŸ‡', 'ğŸŒ¼', 'ğŸ‘', 'ğŸ±', 'ğŸ¶', 'âœ', 'ğŸ†—', 'ğŸŒœ', 'ğŸŒ˜', 'ğŸ˜‡', 'âœˆ', 'ğŸš€', 'ğŸˆ', 'ğŸ‘…', 'ğŸ†’', '8D ', 'ğŸ’', 'ğŸŒ”', 'ğŸ©', 'ğŸš¤', 'ğŸ ', 'ğŸŒŠ', 'ğŸ’–', 'ğŸ¸', 'ğŸ¤—', 'ğŸ˜¹', ':) ', 'ğŸ»', ':)', 'ğŸ’’', 'ğŸœ', 'âœŒ', 'â­ï¸', 'ğŸ’ª', 'ğŸ¤', 'â™•', 'ğŸšŒ', 'ğŸ‘¸', 'ğŸ§', 'ğŸ’‘', 'ğŸ˜„', 'â›µ', 'ğŸ‘„', 'ğŸµ', 'ğŸ‡', 'â˜•', 'ğŸ·', 'ğŸ’—', 'ğŸƒ', 'ğŸ‘Ÿ', 'ğŸŒ‰', 'ğŸ‘‹', ':o)', 'ğŸ¡', 'ğŸ', 'ğŸ˜Š', 'ğŸ·', 'ğŸ¤', 'ğŸŒ’', 'ğŸ©', ':c)', 'ğŸš‚', 'ğŸ€', 'ğŸ©', 'ğŸ˜€', 'ğŸ’™', 'ğŸ„', 'âš“', 'ğŸŒº', 'ğŸ¤', 'ğŸ‘¢', 'ğŸ', 'â™¥', 'ğŸ¤', 'ğŸ’', 'ğŸ’œ', ':-D', 'ğŸ¸', 'âœ¿', 'ğŸ‘¯', 'ğŸ“', 'ğŸ¤', 'ğŸ–¤', 'ğŸ‚', 'ğŸ‘”', 'ğŸŒ¿', 'ğŸ‘­', 'âœ…', 'ğŸ†', 'ğŸ•', 'ğŸ¬', 'âœ¨', 'ğŸµ', 'ğŸƒ', 'ğŸŒ‡', 'ğŸ«', 'ğŸŒ»', 'â›„', 'ğŸ…', 'ğŸ‡', 'ğŸ´', 'ğŸ—½', 'âœ’', 'ğŸ“£', 'ğŸ¥§', 'ğŸ’', 'ğŸ‘¶', 'XD', 'ğŸ­', 'ğŸ¤©', 'ğŸª', 'âœŠ', 'ğŸ‘š', ':*', 'ğŸ¨', '8-D', 'ğŸ’¡', 'â˜»', 'ğŸ§', 'ğŸ’»', 'ğŸŒ¸', 'ğŸŒŸ', 'ğŸŠ', 'ğŸ’˜', 'â™©', 'ğŸ’', '=]', 'ğŸ’‡', 'ğŸ‚', 'ğŸ”', 'ğŸ‘—', 'â˜†', 'x-D', 'ğŸš“', ';-)', 'ğŸ‘ ', 'ğŸ‹', ':3', 'ğŸ§', 'ğŸ˜‹', 'ğŸš¨', ';-]', 'ğŸ“š', 'ğŸŒ³', 'X-D', 'ğŸ‚', 'ğŸš—', ':->', 'ğŸ‘Œ', 'ğŸº', 'ğŸ€', 'ğŸ¦', ';Ë†)', 'ğŸ’“', 'ğŸ˜œ', 'ğŸš²', 'ğŸ‘°', 'ğŸ””', 'ğŸ¡', 'ğŸ½', 'ğŸŒ¹', 'ğŸª', 'ğŸ’«', 'ğŸ˜ƒ', ':â€™-)', 'â™¬', 'ğŸ“·', 'ğŸ¶', 'ğŸ˜™', ': )', 'ğŸ˜›', 'ğŸ‘•', 'ğŸƒ', 'ğŸš', 'ğŸš£', 'ğŸ®', 'ğŸ‘¡', 'ğŸ—¼', 'ğŸ', 'ğŸ˜º', 'ğŸ†', 'ğŸŒ²', 'â˜¼', 'ğŸš´', 'ğŸ', '']\n"
     ]
    }
   ],
   "source": [
    "print(pos_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':<', 'ğŸ‘¿', 'ğŸ˜¬', ':-[', 'ğŸ’€', ':-(', 'ğŸ˜·', 'ğŸ˜±', ':-c', 'ğŸ˜¾', ':(', 'ğŸ', 'ğŸ†˜', 'ğŸ˜¢', 'D8', 'ğŸ˜ª', '>:\\\\', ':-.', 'ğŸ”', '>:(', ':c', 'ğŸ™', 'ğŸ˜´', 'ğŸ˜‘', 'â˜ ', 'ğŸ˜’', 'ğŸ’‰', 'ğŸ˜«', ':@', 'ğŸ˜¥', 'ğŸŒµ', 'ğŸ˜ ', 'âŒ›', 'ğŸ˜³', 'ğŸ”¨', 'ğŸ˜¤', 'â“', 'ğŸ’£', 'ğŸ”™', 'ğŸ˜²', 'ğŸ¤’', ':/', 'ğŸŒ‹', 'ğŸ˜Ÿ', 'ğŸ‘®', ':\\\\', ':â€™(', 'ğŸ‘', 'â›”', 'DX', 'ğŸš‘', '>:[', 'ğŸ”“', 'ğŸ˜¯', 'ğŸ¤•', 'ğŸ¤¢', '=/', 'ğŸ”±', 'ğŸ˜©', ':[', ':-<', 'ğŸ’”', 'ğŸ˜¼', 'ğŸ´', ':-/', 'â—', 'ğŸ˜®', 'ğŸš«', 'âš ', 'ğŸ™‡', 'ğŸ™', 'âœ‚', 'ğŸ“', ':S', 'ğŸ˜–', 'ğŸ–•', '<\\\\3', 'D-â€™:', 'v.v', 'ğŸ˜¡', 'ğŸ˜”', 'ğŸ˜¦', 'ğŸ˜µ', '>.<', 'ğŸ“Œ', 'ğŸ¤®', '</3', 'ğŸ‘º', 'ğŸ¤§', 'D;', 'ğŸ˜°', 'ğŸ¥€', 'ğŸ˜£', '=L', 'ğŸ˜¨', ': (', 'ğŸ˜­', '=\\\\', 'ğŸ”ª', 'ğŸ˜', 'D=', 'ğŸ˜', 'ğŸ²', ':L', 'ğŸ˜§', ':-||', 'ğŸ˜¿', 'ğŸ', 'ğŸ¤¬', 'ğŸ˜“', 'ğŸ˜•', ':{', 'ğŸ˜¶', 'D:', 'â©', ' >:/', 'ğŸ¥µ', 'ğŸ™', 'D:<', 'â†ª', 'ğŸ‘', 'â˜¹', 'ğŸ¤', '']\n"
     ]
    }
   ],
   "source": [
    "print(neg_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: pos 76906 neg 72905\n"
     ]
    }
   ],
   "source": [
    "pos_file = 'arabic_tweets_tsv/20191104/Arabic_tweets_positive_20191104.tsv'\n",
    "neg_file = 'arabic_tweets_tsv/20191104/Arabic_tweets_negative_20191104.tsv'\n",
    "pos_data, pos_labels = read_tsv(pos_file)\n",
    "neg_data, neg_labels = read_tsv(neg_file)\n",
    "print('before:', 'pos', len(pos_data), 'neg', len(neg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: pos 75478 neg 68814 mix 5519\n"
     ]
    }
   ],
   "source": [
    "all_tweets = label_mix(pos_data, neg_data)\n",
    "outfile_name = 'arabic_tweets_tsv/20191104/3labels/Arabic_tweets_20191104.tsv'\n",
    "write_tsv(outfile_name, all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\tÙ„Ø§ Ø³Ù„Ø·Ø© Ù„Ù†Ø§ Ø¹Ù„ÙŠ Ù‚Ù„ÙˆØ¨Ù†Ø§ Ù‡ÙŠ ØªÙ†Ø¨Ø¶ Ù„Ù…Ù† Ø§Ø±Ø§Ø¯Øª ÙˆÙ…ØªÙŠ Ø§Ø±Ø§Ø¯Øª .. ÙˆÙƒÙŠÙ Ø§Ø±Ø§Ø¯Øª .. Ø¨Ø¹Ø¶Ù‡Ù… ÙŠÙ†Ø¨Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ù„Ù‡ .. ÙˆØ¨Ø¹Ø¶Ù‡Ù… ÙŠÙ†Ø¨Ø¶ Ø§Ù„Ù‚â€¦\n",
      "\n",
      "----------------------\n",
      "neg\tÙŠØ§Ø±Ø¨ÙŠ Ù…ØªÙ‰ ØªÙˆØµÙ„ Ù„Ù„Ù…Ù„ÙŠØ§Ø±ğŸ˜\n",
      "\n",
      "----------------------\n",
      "pos\t: #ØµØ¨Ø§Ø­ÙŠØ§Øª ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© ÙˆØªÙØ§Ø¤Ù„ ÙŠØ§Ø±Ø¨ ğŸ’™\n",
      "\n",
      "----------------------\n",
      "neg\tÙ…Ø®Ù„Ø§Ø§Øµ Ø¨Ù‚Ø§Ø§ ÙŠÙ„Ø¹Ù† Ù…ÙŠØªÙŠÙ† Ø§Ù…Ùƒ\n",
      "\n",
      "----------------------\n",
      "pos\tÙˆØ¨Ùƒ ÙˆØ·Ù† ÙŠÙ†Ù‡Ø¶ Ø³ÙŠØ¯ÙŠ Ù‚Ø§Ø¨ÙˆØ³ â¤ #ØµØ¨Ø§Ø­_Ù†ÙˆÙÙ…Ø¨Ø±ÙŠ â¦ğŸ‡´ğŸ‡²â©\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(all_tweets[i])\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: pos 29849 neg 28902\n"
     ]
    }
   ],
   "source": [
    "pos_file = 'arabic_tweets_tsv/20190413/Arabic_tweets_positive_20190413.tsv'\n",
    "neg_file = 'arabic_tweets_tsv/20190413/Arabic_tweets_negative_20190413.tsv'\n",
    "pos_data, pos_labels = read_tsv(pos_file)\n",
    "neg_data, neg_labels = read_tsv(neg_file)\n",
    "print('before:', 'pos', len(pos_data), 'neg', len(neg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: pos 29460 neg 27037 mix 2254\n"
     ]
    }
   ],
   "source": [
    "all_tweets = label_mix(pos_data, neg_data)\n",
    "outfile_name = 'arabic_tweets_tsv/20190413/3labels/Arabic_tweets_20190413.tsv'\n",
    "write_tsv(outfile_name, all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\tğŸ’¥ Ù‚Ø±Ø¨ Ø´Ù‡Ø± Ø§Ù„Ø®ÙŠØ± ğŸŒ™ ÙˆØµØ§Ø± Ù„Ø§Ø²Ù…Ù‡Ø§ Ø¹Ø±ÙˆÙˆØ¶\n",
      "\n",
      "----------------------\n",
      "neg\t#Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ -ØªØ­Øª Ù‡Ø°ÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ù‡ ØªØ­Ø¯ÙŠ Ø±ÙˆÙ‚Ø§Ù† ÙˆØ§Ø³Ø±Ø¹ Ø§Ø°Ø§ ÙØ§Ø² Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ ØŒ Ø§Ø³Ø±Ø¹ Ø±Ø§Ø­ ÙŠÙˆØ²Ø¹ Ù‚ÙŠÙ…Ø© Ø§ÙŠÙÙˆÙ† X â€¦\n",
      "\n",
      "----------------------\n",
      "pos\tØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±Ø§Øª ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† ÙˆØ§Ù„ØµÙ„Ø§Ø© Ø¹Ù„Ù‰ Ù…Ø­Ù…Ø¯ ÙˆØ§Ù„ Ù…Ø­Ù…Ø¯ ğŸŒ· ğŸ’• ğŸŒ·\n",
      "\n",
      "----------------------\n",
      "mix\tğŸ˜‚ğŸ˜‚ ÙÙƒØ±ÙŠ Ø¨Ø¹Ø¯ Ù…Ø§ Ø¨ØºÙŠØ± Ø§Ø³Ù…ÙŠ ğŸ˜• Ù‡Ø°Ø§ Ø¹Ù…Ø± ÙŠØ¬ÙŠØ¨ Ù‡Ø§Ù„Ø¨Ù„Ø§ÙˆÙŠ ğŸ˜‚ğŸ˜‚ğŸ‘Š\n",
      "\n",
      "----------------------\n",
      "neg\tØ°ÙƒØ±ØªÙƒ ÙŠÙˆÙ… Ø£Ù…Ø·Ø±Øª Ø¯Ø§Ø±Ùƒ ÙˆØ£Ù†Ø§ Ø¨Ø¹ÙŠØ¯ ÙˆÙ‡Ù„Øª Ø¯Ù…ÙˆØ¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯ Ø¹Ø¨Ø±Ø§ØªÙ‡Ø§ ğŸ’” #Ø¨Ù‚Ù„Ù…ÙŠ #Ø¬Ø¯ÙŠØ¯ #Ø§Ù…Ø·Ø§Ø±_Ø§Ù„Ø´Ø±Ù‚ÙŠÙ‡\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(all_tweets[i])\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
